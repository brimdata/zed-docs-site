"use strict";(self.webpackChunkzed_docs=self.webpackChunkzed_docs||[]).push([[5483],{3905:function(e,t,n){n.d(t,{Zo:function(){return d},kt:function(){return m}});var a=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var s=a.createContext({}),c=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},d=function(e){var t=c(e.components);return a.createElement(s.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},p=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,i=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),p=c(n),m=o,h=p["".concat(s,".").concat(m)]||p[m]||u[m]||i;return n?a.createElement(h,r(r({ref:t},d),{},{components:n})):a.createElement(h,r({ref:t},d))}));function m(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=n.length,r=new Array(i);r[0]=p;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:o,r[1]=l;for(var c=2;c<i;c++)r[c]=n[c];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}p.displayName="MDXCreateElement"},3204:function(e,t,n){n.r(t),n.d(t,{assets:function(){return d},contentTitle:function(){return s},default:function(){return m},frontMatter:function(){return l},metadata:function(){return c},toc:function(){return u}});var a=n(7462),o=n(3366),i=(n(7294),n(3905)),r=["components"],l={},s="Zed Lake Design Notes",c={unversionedId:"zed/design",id:"zed/design",title:"Zed Lake Design Notes",description:"* Cloud Object Architecture",source:"@site/docs/zed/design.md",sourceDirName:"zed",slug:"/zed/design",permalink:"/docs/zed/design",editUrl:"https://github.com/brimdata/zed-docs/tree/main/docs/zed/design.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Zed lake service API",permalink:"/docs/zed/api"},next:{title:"zq - command-line Zed query",permalink:"/docs/zq/"}},d={},u=[{value:"Cloud Object Architecture",id:"cloud-object-architecture",level:2},{value:"Immutable Objects",id:"immutable-objects",level:3},{value:"Data Objects",id:"data-objects",level:4},{value:"Commit History",id:"commit-history",level:4},{value:"Transaction Journal",id:"transaction-journal",level:3},{value:"Scaling a Journal",id:"scaling-a-journal",level:4},{value:"Journal Concurrency Control",id:"journal-concurrency-control",level:4},{value:"Configuration State",id:"configuration-state",level:4},{value:"Merge on Read",id:"merge-on-read",level:3},{value:"Object Naming",id:"object-naming",level:3},{value:"Continuous Ingest",id:"continuous-ingest",level:2},{value:"Derived Analytics",id:"derived-analytics",level:2},{value:"Keyless Data",id:"keyless-data",level:2},{value:"Relational Model",id:"relational-model",level:2},{value:"Type Rule",id:"type-rule",level:2},{value:"Aggregation Rule",id:"aggregation-rule",level:2},{value:"Vacuum Support",id:"vacuum-support",level:2}],p={toc:u};function m(e){var t=e.components,n=(0,o.Z)(e,r);return(0,i.kt)("wrapper",(0,a.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"zed-lake-design-notes"},"Zed Lake Design Notes"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#cloud-object-architecture"},"Cloud Object Architecture"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#immutable-objects"},"Immutable Objects"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#data-objects"},"Data Objects")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#commit-history"},"Commit History")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#transaction-journal"},"Transaction Journal"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#scaling-a-journal"},"Scaling a Journal")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#journal-concurrency-control"},"Journal Concurrency Control")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#configuration-state"},"Configuration State")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#merge-on-read"},"Merge on Read")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#object-naming"},"Object Naming")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#continuous-ingest"},"Continuous Ingest")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#derived-analytics"},"Derived Analytics")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#keyless-data"},"Keyless Data")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#relational-model"},"Relational Model")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#type-rule"},"Type Rule")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#aggregation-rule"},"Aggregation Rule")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"#vacuum-support"},"Vacuum Support"))),(0,i.kt)("h2",{id:"cloud-object-architecture"},"Cloud Object Architecture"),(0,i.kt)("p",null,"The Zed lake semantics are achieved by mapping the\nlake and pool abstractions onto a key-value cloud object store."),(0,i.kt)("p",null,"Every data element in a Zed lake is either of two fundamental object types:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"a single-writer ",(0,i.kt)("em",{parentName:"li"},"immutable object"),", or"),(0,i.kt)("li",{parentName:"ul"},"a multi-writer ",(0,i.kt)("em",{parentName:"li"},"transaction journal"),".")),(0,i.kt)("h3",{id:"immutable-objects"},"Immutable Objects"),(0,i.kt)("p",null,"All imported data in a data pool is composed of immutable objects, which are organized\naround a primary data object.  Each data object is composed of one or more immutable objects\nall of which share a common, globally unique identifier,\nwhich is referred to below generically as ",(0,i.kt)("inlineCode",{parentName:"p"},"<id>")," below."),(0,i.kt)("p",null,"These identifiers are ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/segmentio/ksuid"},"KSUIDs"),".\nThe KSUID allocation scheme\nprovides a decentralized solution for creating globally unique IDs.\nKSUIDs have embedded timestamps so the creation time of\nany object named in this way can be derived.  Also, a simple lexicographic\nsort of the KSUIDs results in a creation-time ordering (though this ordering\nis not relied on for causal relationships since clock skew can violate\nsuch an assumption)."),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"While a Zed lake is defined in terms of a cloud object store, it may also\nbe realized on top of a file system, which provides a convenient means for\nlocal, small-scale deployments for test/debug workflows.  Thus, for simple use cases,\nthe complexity of running an object-store service may be avoided.")),(0,i.kt)("h4",{id:"data-objects"},"Data Objects"),(0,i.kt)("p",null,"An immutable object is created by a single writer using a globally unique name\nwith an embedded KSUID.",(0,i.kt)("br",{parentName:"p"}),"\n","New objects are written in their entirety.  No updates, appends, or modifications\nmay be made once an object exists.  Given these semantics, any such object may be\ntrivially cached as neither its name nor content ever change."),(0,i.kt)("p",null,"Since the object's name is globally unique and the\nresulting object is immutable, there is no possible write concurrency to manage\nwith respect to a given object."),(0,i.kt)("p",null,"A data object is composed of"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"the primary data object stored as one or two objects (for row and/or column layout),"),(0,i.kt)("li",{parentName:"ul"},"an optional seek index, and"),(0,i.kt)("li",{parentName:"ul"},"zero or more search indexes.")),(0,i.kt)("p",null,"Data objects may be either in row form (i.e., ZNG) or column form (i.e., ZST),\nor both forms may be present as a query optimizer may choose to use whatever\nrepresentation is more efficient.\nWhen both row and column data objects are present, they must contain the same\nunderlying Zed data."),(0,i.kt)("p",null,"Immutable objects are named as follows:"),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null},"object type"),(0,i.kt)("th",{parentName:"tr",align:null},"name"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"column data"),(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"<pool-id>/data/<id>.zst"))),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"row data"),(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"<pool-id>/data/<id>.zng"))),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"row seek index"),(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"<pool-id>/data/<id>-seek.zng"))),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"search index"),(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"<pool-id>/index/<id>-<index-id>.zng"))))),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"<id>")," is the KSUID of the data object.\n",(0,i.kt)("inlineCode",{parentName:"p"},"<index-id>")," is the KSUID of an index object created according to the\nindex rules described above.  Every index object is defined\nwith respect to a data object."),(0,i.kt)("p",null,"The seek index maps pool key values to seek offsets in the ZNG file thereby\nallowing a scan to do a partial GET of the ZNG object when scanning only\na subset of data."),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"Note the ZST format will have seekable checkpoints based on the sort key that\nare encoded into its metadata section so there is no need to have a separate\nseek index for the columnar object.")),(0,i.kt)("h4",{id:"commit-history"},"Commit History"),(0,i.kt)("p",null,"A pool's commit history is the definitive record of the evolution of data in\nthat pool in a transactionally consistent fashion."),(0,i.kt)("p",null,"Each commit object entry is identified with its ",(0,i.kt)("inlineCode",{parentName:"p"},"commit ID"),".\nObjects are immutable and uniquely named so there is never a concurrent write\ncondition."),(0,i.kt)("p",null,'The "add" and "commit" operations are transactionally stored\nin a chain of commit objects.  Any number of adds (and deletes) may appear\nin a commit object.  All of the operations that belong to a commit are\nidentified with a commit identifier (ID).'),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},'TBD: when a scan encounters an object that was physically deleted for\nwhatever reason, it should simply continue on and issue a warning on\nthe query endpoint "warnings channel".')),(0,i.kt)("p",null,"As each commit object points to its parent (except for the initial commit\nin main), the collection of commit objects in a pool forms a tree."),(0,i.kt)("p",null,"Each commit object contains a sequence of ",(0,i.kt)("em",{parentName:"p"},"actions"),":"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"Add")," to add a data object reference to a pool,"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"Delete")," to delete a data object reference from a pool,"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"AddIndex")," to bind an index object to a data object to prune the data object\nfrom a scan when possible using the index,"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"DeleteIndex")," to remove an index object reference to its data object, and"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"Commit")," for providing metadata about each commit.")),(0,i.kt)("p",null,"The actions are not grouped directly by their commit tag but instead each\naction embeds the KSUID of its commit tag."),(0,i.kt)("p",null,"By default, ",(0,i.kt)("inlineCode",{parentName:"p"},"zed log")," outputs an abbreviated form of the log as text to\nstdout, similar to the output of ",(0,i.kt)("inlineCode",{parentName:"p"},"git log"),"."),(0,i.kt)("p",null,"However, the log represents the definitive record of a pool's present\nand historical content, and accessing its complete detail can provide\ninsights about data layout, provenance, history, and so forth.  Thus, the\nZed lake provides a means to query a pool's configuration state as well,\nthereby allowing past versions of the complete pool and branch configurations\nas well as all of their underlying data to be subject to time travel.\nTo interrogate the underlying transaction history of the branches and\ntheir pointers, simply query a pool's \"branchlog\" via the syntax ",(0,i.kt)("inlineCode",{parentName:"p"},"<pool>:branchlog"),"."),(0,i.kt)("p",null,"For example, to aggregate a count of each journal entry type of the pool\ncalled ",(0,i.kt)("inlineCode",{parentName:"p"},"logs"),", you can simply say:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'zed query "from logs:branchlog | count() by typeof(this)"\n')),(0,i.kt)("p",null,'Since the Zed system "typedefs" each journal record with a named type,\nthis kind of query gives intuitive results.  There is no need to implement\na long list of features for journal introspection since the data in its entirety\ncan be simply and efficiently processed as a ZNG stream.'),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"Note that the branchlog meta-query source is not yet implemented.")),(0,i.kt)("h3",{id:"transaction-journal"},"Transaction Journal"),(0,i.kt)("p",null,"State that is mutable is built upon a transaction journal of immutable\ncollections of entries.  In this way, there are no objects in the\nstorage footprint that are ever modified.  Instead, the journal captures\nchanges and journal snapshots are used to provide synchronization points\nfor efficient access to the journal (so the entire journal need not be\nread to create the current state) and old journal entries may be removed\nbased on retention policy."),(0,i.kt)("p",null,"The journal may be updated concurrently by multiple writers so concurrency\ncontrols are included (see ",(0,i.kt)("a",{parentName:"p",href:"#journal-concurrency-control"},"Journal Concurrency Control"),"\nbelow) to provide atomic updates."),(0,i.kt)("p",null,'A journal entry simply contains actions that modify the visible "state" of\nthe pool by changing branch name to commit object mappings.  Note that\nadding a commit object to a pool changes nothing until a branch pointer\nis mutated to point at that object.'),(0,i.kt)("p",null,"Each atomic journal commit object is a ZNG file numbered 1 to the end of journal (HEAD),\ne.g., ",(0,i.kt)("inlineCode",{parentName:"p"},"1.zng"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"2.zng"),", etc., each number corresponding to a journal ID.\nThe 0 value is reserved as the null journal ID.\nThe journal's TAIL begins at 1 and is increased as journal entries are purged.\nEntries are added at the HEAD and removed from the TAIL.\nOnce created, a journal entry is never modified but it may be deleted and\nnever again allocated.\nThere may be 1 or more entries in each commit object."),(0,i.kt)("p",null,"Each journal entry implies a snapshot of the data in a pool.  A snapshot\nis computed by applying the transactions in sequence from entry TAIL to\nthe journal entry in question, up to HEAD.  This gives the set of commit tags\nthat comprise a snapshot."),(0,i.kt)("p",null,"The set of branch pointers in a pool is assembled at any point in the journal's history\nby scanning a journal that includes ADD, UPDATE, and DELETE actions for the\nmapping of a branch name to a commit object.  A timestamp is recorded in\neach action to provide for time travel."),(0,i.kt)("p",null,'For efficiency, a journal entry\'s snapshot may be stored as a "cached snapshot"\nalongside the journal entry.  This way, the snapshot at HEAD may be\nefficiently computed by locating the most recent cached snapshot and scanning\nforward to HEAD.'),(0,i.kt)("h4",{id:"scaling-a-journal"},"Scaling a Journal"),(0,i.kt)("p",null,'When the sizes of the journal snapshot files exceed a certain size\n(and thus becomes too large to conveniently handle in memory),\nthe snapshots can be converted to and stored\nin an internal sub-pool called the "snapshot pool".  The snapshot pool\'s\npool key is the "from" value (of its parent pool key) from each commit action.\nIn this case, commits to the parent pool are made in the same fashion,\nbut instead of snapshotting updates into a snapshot ZNG file,\nthe snapshots are committed to the journal sub-pool.  In this way, commit histories\ncan be rolled up and organized by the pool key.  Likewise, retention policies\nbased on the pool key can remove not just data objects from the main pool but\nalso data objects in the journal pool comprising committed data that falls\noutside of the retention boundary.'),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"Note we currently record a delete using only the object ID.  In order to\norganize add and delete actions around key spans, we need to add the span\nmetadata to the delete action just as it exists in the add action.")),(0,i.kt)("h4",{id:"journal-concurrency-control"},"Journal Concurrency Control"),(0,i.kt)("p",null,"To provide for atomic commits, a writer must be able to atomically update\nthe HEAD of the log.  There are three strategies for doing so."),(0,i.kt)("p",null,'First, if the cloud service offers "put-if-missing" semantics, then a writer\ncan simply read the HEAD file and use put-if-missing to write to the\njournal at position HEAD+1.  If this fails because of a race, then the writer\ncan simply write at position HEAD+2 and so forth until it succeeds (and\nthen update the HEAD object).  Note that there can be a race in updating\nHEAD, but HEAD is always less than or equal to the real end of journal,\nand this condition can be self-corrected by probing for HEAD+1 whenever\nthe HEAD of the journal is accessed.'),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"Note that put-if-missing can be emulated on a local file system by opening\na file for exclusive access and checking that it has zero length after\na successful open.")),(0,i.kt)("p",null,"Second, strong read/write ordering semantics (as exists in Amazon S3)\ncan be used to implement transactional journal updates as follows:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("em",{parentName:"li"},"TBD: this is worked out but needs to be written up"))),(0,i.kt)("p",null,"Finally, since the above algorithm requires many round trips to the storage\nsystem and such round trips can be tens of milliseconds, another approach\nis to simply run a lock service as part of a cloud deployment that manages\na mutex lock for each pool's journal."),(0,i.kt)("h4",{id:"configuration-state"},"Configuration State"),(0,i.kt)("p",null,"Configuration state describing a lake or pool is also stored in mutable objects.\nZed lakes simply use a commit journal to store configuration like the\nlist of pools and pool attributes, indexing rules used across pools,\netc.  Here, a generic interface to a commit journal manages any configuration\nstate simply as a key-value store of snapshots providing time travel over\nthe configuration history."),(0,i.kt)("h3",{id:"merge-on-read"},"Merge on Read"),(0,i.kt)("p",null,"To support ",(0,i.kt)("em",{parentName:"p"},"sorted scans"),",\ndata from overlapping objects is read in parallel and merged in sorted order.\nThis is called the ",(0,i.kt)("em",{parentName:"p"},"merge scan"),"."),(0,i.kt)("p",null,"However, if many overlapping data objects arise, performing a merge scan\non every read can be inefficient.\nThis can arise when\nmany random data ",(0,i.kt)("inlineCode",{parentName:"p"},"load"),' operations involving perhaps "late" data\n(e.g., the pool key is a timestamp and records with old timestamp values regularly\nshow up and need to be inserted into the past).  The data layout can become\nfragmented and less efficient to scan, requiring a scan to merge data\nfrom a potentially large number of different objects.'),(0,i.kt)("p",null,"To solve this problem, the Zed lake design follows the\n",(0,i.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Log-structured_merge-tree"},"LSM")," design pattern.\nSince records in each data object are stored in sorted order, a total order over\na collection of objects (e.g., the collection coming from a specific set of commits)\ncan be produced by executing a sorted scan and rewriting the results back to the pool\nin a new commit.  In addition, the objects comprising the total order\ndo not overlap.  This is just the basic LSM algorithm at work."),(0,i.kt)("p",null,"To perform an LSM rollup, the ",(0,i.kt)("inlineCode",{parentName:"p"},"compact")," command (implementation tracked\nvia ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/brimdata/zed/issues/2977"},"zed/2977"),')\nis like a "squash" to perform LSM-like compaction function, e.g.,'),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"zed compact <id> [<id> ...]\n(merged commit <id> printed to stdout)\n")),(0,i.kt)("p",null,"After compaction, all of the objects comprising the new commit are sorted\nand non-overlapping.\nHere, the objects from the given commit IDs are read and compacted into\na new commit.  Again, until the data is actually committed,\nno readers will see any change."),(0,i.kt)("p",null,"Unlike other systems based on LSM, the rollups here are envisioned to be\nrun by orchestration agents operating on the Zed lake API.  Using\nmeta-queries, an agent can introspect the layout of data, perform\nsome computational geometry, and decide how and what to compact.\nThe nature of this orchestration is highly workload dependent so we plan\nto develop a family of data-management orchestration agents optimized\nfor various use cases (e.g., continuously ingested logs vs. collections of\nmetrics that should be optimized with columnar form vs. slowly-changing\ndimensional datasets like threat intel tables)."),(0,i.kt)("p",null,'An orchestration layer outside of the Zed lake is responsible for defining\npolicy over\nhow data is ingested and committed and rolled up.  Depending on the\nuse case and workflow, we envision that some amount of overlapping data objects\nwould persist at small scale and always be "mixed in" with other overlapping\ndata during any key-range scan.'),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"Note: since this style of data organization follows the LSM pattern,\nhow data is rolled up (or not) can control the degree of LSM write\namplification that occurs for a given workload.  There is an explicit\ntradeoff here between overhead of merging overlapping objects on read\nand LSM write amplification to organize the data to avoid such overlaps.")),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"Note: we are showing here manual, CLI-driven steps to accomplish these tasks\nbut a live data pipeline would automate all of this with orchestration that\nperforms these functions via a service API, i.e., the same service API\nused by the CLI operators.")),(0,i.kt)("h3",{id:"object-naming"},"Object Naming"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"<lake-path>/\n  lake.zng\n  pools/\n    HEAD\n    TAIL\n    1.zng\n    2.zng\n    ...\n  index_rules/\n    HEAD\n    TAIL\n    1.zng\n    2.zng\n    ...\n    ...\n  <pool-id-1>/\n    branches/\n      HEAD\n      TAIL\n      1.zng\n      2.zng\n      ...\n    commits/\n      <id1>.zng\n      <id2>.zng\n      ...\n    data/\n      <id1>.{zng,zst}\n      <id2>.{zng,zst}\n      ...\n    index/\n      <id1>-<index-id-1>.zng\n      <id1>-<index-id-2>.zng\n      ...\n      <id2>-<index-id-1>.zng\n      ...\n  <pool-id-2>/\n  ...\n")),(0,i.kt)("h2",{id:"continuous-ingest"},"Continuous Ingest"),(0,i.kt)("p",null,"While the description above is very batch oriented, the Zed lake design is\nintended to perform scalably for continuous streaming applications.  In this\napproach, many small commits may be continuously executed as data arrives and\nafter each commit, the data is immediately readable."),(0,i.kt)("p",null,"To handle this use case, the ",(0,i.kt)("em",{parentName:"p"},"journal")," of branch commits is designed\nto scale to arbitrarily large footprints as described earlier."),(0,i.kt)("h2",{id:"derived-analytics"},"Derived Analytics"),(0,i.kt)("p",null,"To improve the performance of predictable workloads, many use cases of a\nZed lake pre-compute ",(0,i.kt)("em",{parentName:"p"},"derived analytics")," or a particular set of ",(0,i.kt)("em",{parentName:"p"},"partial\naggregations"),"."),(0,i.kt)("p",null,"For example, the Brim app displays a histogram of event counts grouped by\na category over time.  The partial aggregation for such a computation can be\nconfigured to run automatically and store the result in a pool designed to\nhold such results.  Then, when a scan is run, the Zed analytics engine\nrecognizes when the DAG of a query can be rewritten to assemble the\npartial results instead of deriving the answers from scratch."),(0,i.kt)("p",null,"When and how such partial aggregations are performed is simply a matter of\nwriting Zed queries that take the raw data and produce the derived analytics\nwhile conforming to a naming model that allows the Zed lake to recognize\nthe relationship between the raw data and the derived data."),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"TBD: Work out these details which are reminiscent of the analytics cache\ndeveloped in our earlier prototype.")),(0,i.kt)("h2",{id:"keyless-data"},"Keyless Data"),(0,i.kt)("p",null,'This is TBD.  Data without a key should be accepted some way or another.\nOne approach is to simply assign the "zero-value" as the pool key; another\nis to use a configured default value.  This would make key-based\nretention policies more complicated.'),(0,i.kt)("p",null,"Another approach would be to create a sub-pool on demand when the first\nkeyless data is encountered, e.g., ",(0,i.kt)("inlineCode",{parentName:"p"},"pool-name.$nokey"),' where the pool key\nis configured to be "this".  This way, an app or user could query this pool\nby this name to scan keyless data.'),(0,i.kt)("h2",{id:"relational-model"},"Relational Model"),(0,i.kt)("p",null,"Since a Zed lake can provide strong consistency, workflows that manipulate\ndata in a lake can utilize a model where updates are made to the data\nin place.  Such updates involve creating new commits from the old data\nwhere the new data is a modified form of the old data.  This provides\nemulation of row-based updates and deletes."),(0,i.kt)("p",null,'If the pool-key is chosen to be "this" for such a use case, then unique\nrows can be maintained by trivially detected duplicates (because any\nduplicate row will be adjacent when sorted by "this") so that duplicates are\ntrivially detected.'),(0,i.kt)("p",null,"Efficient upserts can be accomplished because each data object is sorted by the\npool key.  Thus, an upsert can be sorted then merge-joined with each\noverlapping object.  Where data objects produce changes and additions, they can\nbe forwarded to a streaming add operator and the list of modified objects\naccumulated.  At the end of the operation, then new commit(s) along with\nthe to-be-deleted objects can be added to the journal in a single atomic\noperation.  A write conflict occurs if there are any other deletes added to\nthe list of to-be-deleted objects.  When this occurs, the transaction can\nsimply be restarted.  To avoid inefficiency of many restarts, an upsert can\nbe partitioned into smaller objects if the use case allows for it."),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"TBD: Finish working through this use case, its requirements, and the\nmechanisms needed to implement it.  Write conflicts will need to be\nmanaged at a layer above the journal or the journal extended with the\nneeded functionality.")),(0,i.kt)("h2",{id:"type-rule"},"Type Rule"),(0,i.kt)("p",null,"A type rule indicates that all values of any field of a specified type\nbe indexed where the type signature uses Zed type syntax.\nFor example,"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"zed index create IndexGroupEx type ip\n")),(0,i.kt)("p",null,"creates a rule that indexes all IP addresses appearing in fields of type ",(0,i.kt)("inlineCode",{parentName:"p"},"ip"),"\nin the index group ",(0,i.kt)("inlineCode",{parentName:"p"},"IndexGroupEx"),"."),(0,i.kt)("h2",{id:"aggregation-rule"},"Aggregation Rule"),(0,i.kt)("p",null,"An aggregation rule allows the creation of any index keyed by one or more fields\n(primary, second, etc.), typically the result of an aggregation.\nThe aggregation is specified as a Zed query.\nFor example,"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'zed index create IndexGroupEx agg "count() by field"\n')),(0,i.kt)("p",null,"creates a rule that creates an index keyed by the group-by keys whose\nvalues are the partial-result aggregation given by the Zed expression."),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"This is not yet implemented.  The query planner would replace any full object\nscan with the needed aggregation with the result given in the index.\nWhere a filter is applied to match one row of the index, that result could\nlikewise be extracted instead of scanning the entire object.\nThis capability is not generally useful for interactive search and analytics\n(except for optimizations that suit the interactive app) but rather is a powerful\ncapability for application-specific workflows that know the pre-computed\naggregations that they will use ahead of time, e.g., beacon analysis\nof network security logs.")),(0,i.kt)("h2",{id:"vacuum-support"},"Vacuum Support"),(0,i.kt)("p",null,"While data objects currently can be deleted from a lake, the underlying data\nis retained to support time travel."),(0,i.kt)("p",null,"The system must also support purging of old data so that retention policies\ncan be implemented."),(0,i.kt)("p",null,"This could be supported with the DANGER-ZONE command ",(0,i.kt)("inlineCode",{parentName:"p"},"zed vacuum"),"\n(implementation tracked in ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/brimdata/zed/issues/2545"},"zed/2545"),").\nThe commits still appear in the log but scans at any time-travel point\nwhere the commit is present will fail to scan the deleted data.\nIn this case, perhaps we should emit a structured Zed error describing\nthe meta-data of the object that was unavailable."),(0,i.kt)("p",null,"Alternatively, old data can be removed from the system using a safer\ncommand (but still in the DANGER-ZONE), ",(0,i.kt)("inlineCode",{parentName:"p"},"zed vacate")," (also\n",(0,i.kt)("a",{parentName:"p",href:"https://github.com/brimdata/zed/issues/2545"},"zed/2545"),") which moves\nthe tail of the commit journal forward and removes any data no longer\naccessible through the modified commit journal."))}m.isMDXComponent=!0}}]);